c({"name": "steal.html.crawl", "params": {"url": {"description": "<p>the starting page to crawl</p>", "type": "Object", "optional": false, "order": 0, "name": "url"}, "opts": {"description": "<p>the location to put the crawled content.</p>", "type": "String|Object", "optional": false, "order": 1, "name": "opts"}}, "ret": {"type": "undefined", "description": ""}, "type": "function", "comment": "<p>Loads an ajax driven page and generates the html for google to crawl. Check out the [ajaxy tutorial] \n for a more complete walkthrough.</p>\n\n<p>This crawler indexes an entire Ajax site.  It</p>\n\n<ol>\n<li>Opens a page in a headless browser.</li>\n<li>Waits until its content is ready.</li>\n<li>Scrapes its contents.</li>\n<li>Writes the contents to a file.</li>\n<li>Adds any links in the page that start with #! to be indexed</li>\n<li>Changes <code>window.location.hash</code> to the next index-able page</li>\n<li><p>Goto #2 and repeats until all pages have been loaded</p>\n\n<h2>2. Wait until content is ready.</h2>\n\n<p>By default, [steal.html] will just wait until all scripts have finished loading\nbefore scraping the page's contents.  To delay this, use\n[steal.html.delay] and [steal.html.ready].</p>\n\n<h2>3. Write the contents to a file.</h2>\n\n<p>You can change where the contents of the file are writen to by changing\nthe second parameter passed to <code>crawl</code>.</p>\n\n<p>By default uses EnvJS, but you can use PhantomJS for more advanced pages:</p></li>\n</ol>\n\n<pre><code class='javascript'>steal('steal/html', function(){\n    steal.html.crawl(\"ajaxy/ajaxy.html\", \n    {\n        out: 'ajaxy/out',\n        browser: 'phantomjs'\n    })\n})</code></pre> ", "parents": ["steal.html"], "src": "steal/html/crawl/crawl.js", "line": 16, "children": []})