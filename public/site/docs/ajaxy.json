c({"name": "ajaxy", "type": "page", "comment": "<p>This tutorial walks you through building a simple widget\nthat listens for changes in the browser location hash\nand updates the content of the page.  It demonstrates how to make\na site Google crawlable and searchable.</p>\n\n<h2>The App</h2>\n\n<p>We'll make a mini app that updates the contents of page with an\nAjax request when a user clicks on a navigation link. Then, we'll make this searchable\nwith the <code>ajaxy/scripts/crawl.js</code> script.</p>\n\n<div class='demo_wrapper' data-demo-src='tutorials/ajaxy/ajaxy.html'></div>\n\n<p>The crawl script generates html pages that Google can use as a representation\nof the content of an Ajax application.  Read Google's documentation on its\n[https://developers.google.com/webmasters/ajax-crawling/docs/getting-started Ajax crawling API]\n before continuing this tutorial.</p>\n\n<h2>Setup</h2>\n\n<p>[installing Download and install] the latest version of JavaScriptMVC.</p>\n\n<p>After installing JavaScriptMVC, open a command line to \nthe [steal.static.root steal.config().root] folder (where you unzipped\nJavaScriptMVC).  </p>\n\n<p>We'll use the application generator to generate an application\nskeleton folder.  Run:</p>\n\n<pre><code>[WINDOWS] &gt; js jmvc/generate/app ajaxy\n[Lin/Mac] &gt; ./js jmvc/generate/app ajaxy\n</code></pre>\n\n<h2>The Code</h2>\n\n<p>In the generated ajaxy folder, you'll find <code>ajaxy.html</code>\nand <code>ajaxy.js</code>.  We'll add a content area\nand few links to \n<code>ajaxy.html</code>.  When we click on  links,\nwe'll make <code>ajaxy.js</code> load content into\nthe content area.</p>\n\n<p>Change <code>ajaxy.html</code> so it looks like:</p>\n\n<pre><code class='xml'>&lt;!DOCTYPE HTML>\n&lt;html lang=\"en\">\n    &lt;head>\n        &lt;title>Ajaxy&lt;/title>\n        &lt;meta name=\"fragment\" content=\"!\">\n    &lt;/head>\n    &lt;body>\n        &lt;a href='#!videos'>Videos&lt;/a>\n        &lt;a href='#!articles'>Articles&lt;/a>\n        &lt;a href='#!images'>Images&lt;/a>\n        &lt;div id='content'>&lt;/div>\n        &lt;script type='text/javascript' \n            src='../steal/steal.js?ajaxy,development'>     \n        &lt;/script>\n    &lt;/body>\n&lt;/html></code></pre>\n\n<p>Notice that the page includes a <code>&lt;meta name=\"fragment\" content=\"!\"&gt;</code>\ntag.  This tells to Google to process <code>ajaxy.html</code> as having Ajax content.</p>\n\n<p>Next, add some content to show when these links are clicked.  Put the following content\nin each file:</p>\n\n<p><strong>ajaxy/fixtures/articles.html</strong></p>\n\n<pre><code class='xml'>&lt;h1>Articles&lt;/h1>\n&lt;p>Some articles.&lt;/p></code></pre>\n\n<p><strong>ajaxy/fixtures/images.html</strong></p>\n\n<pre><code class='xml'>&lt;h1>Images&lt;/h1>\n&lt;p>Some images.&lt;/p></code></pre>\n\n<p><strong>ajaxy/fixtures/videos.html</strong></p>\n\n<pre><code class='xml'>&lt;h1>Videos&lt;/h1>\n&lt;p>Some videos.&lt;/p></code></pre>\n\n<p>Finally, change <code>ajaxy.js</code> to look like:</p>\n\n<pre><code>steal('jquery',\n      'can/construct/proxy',\n      'can/control',\n      'can/route',\n      'steal/html',\n      function($, can){\n\nvar Ajaxy = can.Control({\n    \"{route} change\" : function(route, ev){\n        this.updateContent(route.page)\n    },\n    updateContent : function(hash){\n        // postpone reading the html \n        steal.html.wait();\n\n        $.get(\"fixtures/\" + hash + \".html\", {}, this.proxy('replaceContent'), \"text\")\n    },\n    replaceContent : function(html){\n        this.element.html(html);\n\n        // indicate the html is ready to be crawled\n        steal.html.ready();\n    }\n})\n\nnew Ajaxy('#content', { route: can.route(\":page\", { page: \"videos\" }) });\n\n});\n</code></pre>\n\n<p>When a route (<code>\"{route} change\"</code>) event occurs, Ajaxy\nuses the <code>route.page</code> value to make a \nrequest (<code>$.get</code>)\nfor content in the<code>fixtures</code> folder.  For more information\non routing, visit [can.route].</p>\n\n<p>When the content is retrieved, it replaces the element's \nhtml (<code>this.element.html(...)</code>).</p>\n\n<p>Ajaxy also calls <code>updateContent</code> to load content when\nthe page loads initially. </p>\n\n<h2>Crawling and scraping</h2>\n\n<p>To crawl your site and generate google-searchable html, run:</p>\n\n<pre><code class='none'>[WINDOWS] > js ajaxy\\scripts\\crawl.js\n[Lin/Mac] > ./js ajaxy/scripts/crawl.js</code></pre>\n\n<p>This script peforms the following actions:</p>\n\n<ol>\n<li>Opens a page in a headless browser.</li>\n<li>Waits until its content is ready.</li>\n<li>Scrapes its contents.</li>\n<li>Writes the contents to a file.</li>\n<li>Adds any links in the page that start with #! to be indexed</li>\n<li>Changes the url hash to the next index-able page</li>\n<li>Goto #2 and repeats until all pages have been loaded</li>\n</ol>\n\n<h2>Pausing the html scraping.</h2>\n\n<p>By default, the contents are scraped immediately after the page's scripts have loaded or\nthe route has changed.  The Ajax request for content\nhappens asynchronously so we have to tell [steal.html] to wait to scrape the content.</p>\n\n<p>To do this, Ajaxy calls:</p>\n\n<pre><code>steal.html.wait();\n</code></pre>\n\n<p>before the Ajax request.  And when the page is ready, Ajaxy calls:</p>\n\n<pre><code>steal.html.ready();\n</code></pre>\n\n<h2>Getting Google To Crawl Your Site</h2>\n\n<p>If you haven't already, read up on \nGoogle's [https://developers.google.com/webmasters/ajax-crawling/docs/getting-started Ajax crawling API].</p>\n\n<p>When google wants to crawl your site, it will send a \nrequest to your page with <code>_escaped_fragment=</code>.  </p>\n\n<p>When your server sees this param, redirect google to the generated html page.  For example, when the Google Spider requests <code>http://mysite.com?_escaped_fragment=val</code>, this is its attempt to crawl <code>http://mysite.com#!val</code>.  You should redirect this request to <code>http://mysite.com/html/val.html</code>.</p>\n\n<p>Yes, it's that easy!</p>\n\n<h2>Phantom for Advanced Pages</h2>\n\n<p>By default the crawl script uses EnvJS to open your page and build a static snapshot.  For some pages, EnvJS won't be powerful enough to accurately simulate everything.  If your page experiences errors, you can use PhantomJS (headless Webkit) to generate snapshots instead, which may work better.</p>\n\n<p>To turn on Phantom:</p>\n\n<ol>\n<li>Install it using the install instructions [funcunit.phantomjs here]</li>\n<li><p>Open scripts/crawl.js and change the second parameter of steal.html.crawl to an options object with a browser option, like this:</p>\n\n<p>steal('steal/html', function(){\n    steal.html.crawl(\"ajaxy/ajaxy.html\", \n    {\n        out: 'ajaxy/out',\n        browser: 'phantomjs'\n    })\n})</p></li>\n</ol>", "title": "Searchable Ajax Apps", "parents": ["tutorials"], "order": 8, "src": "tutorials/ajaxy/ajaxy.md", "children": []})